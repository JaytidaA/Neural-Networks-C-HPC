% !TEX program = lualatex

\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bookmark}
\usepackage{cleveref}
\usepackage{enumitem}
\usepackage{float}
\usepackage{graphicx}
\usepackage[a4paper, margin=1 in, bindingoffset=0.5in]{geometry}
\usepackage{hyperref}
\usepackage{listings}
\usepackage[indent=0pt]{parskip}
\usepackage{sourcecodepro}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codeorange}{rgb}{0.86,0.41,0.26}
\definecolor{codeblue}{rgb}{0.16,0.43,0.65}
%\definecolor{backcolour}{rgb}{0.12,0.12,0.12}
\definecolor{js}{RGB}{240, 219, 79}
\definecolor{fedora}{RGB}{41, 65, 114}

\lstdefinestyle{general}{
	%backgroundcolor=\color{backcolour},
	commentstyle=\small\color{codegreen},
	keywordstyle=\small\color{codeblue},
	numberstyle=\ttfamily\small\color{codegray},
	stringstyle=\small\color{codeorange},
	basicstyle=\ttfamily\small,
	breaklines=true,
	frame=single,
	keepspaces=true,
	numbers=left,
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\title{Assignment V: Implementation of an Artificial Neural Network}
\author{Aaditya Joil (Jojo)\\Batch - B, 2310800\textcolor{red}{\texttt{33}}}
\date{\today}

\begin{document}
\maketitle

\section*{Aim}
To implement a basic feed-forward Artificial Neural Network (ANN) classifier with backpropagation and compare its performance with the corresponding model from \texttt{scikit-learn}.

\section*{Theory}
Machine Learning models can be broadly placed on a spectrum from simple linear models to highly expressive non-linear architectures that can approximate extremely complex functions. Artificial Neural Networks fall into the latter category: instead of relying on a single linear transformation, they compose many simple transformations (layers) and non-linear activation functions to approximate rich decision boundaries in high-dimensional spaces.

Conceptually, an ANN is inspired by biological neural systems but is mathematically very clean. Each neuron computes a weighted sum of its inputs, adds a bias, and passes the result through a non-linear function. By stacking many such layers and adjusting the weights using data-driven optimisation (typically gradient descent), the network gradually learns to minimise a chosen loss function and perform tasks such as classification or regression.

\section*{Core}
At the heart of a feed-forward ANN is the transformation of an input vector $x \in \mathbb{R}^d$ through one or more hidden layers into an output vector representing class scores or probabilities.

\subsection*{Forward Pass}
Consider a network with one hidden layer for a $K$-class classification problem. Let the hidden layer have $H$ neurons. Then:
\[
z^{(1)} = W^{(1)} x + b^{(1)}, \quad a^{(1)} = \sigma\!\left(z^{(1)}\right)
\]
\[
z^{(2)} = W^{(2)} a^{(1)} + b^{(2)}, \quad \hat{y} = \text{softmax}\!\left(z^{(2)}\right)
\]
where:
\begin{itemize}
  \item $W^{(1)} \in \mathbb{R}^{H \times d}$ and $b^{(1)} \in \mathbb{R}^{H}$ are the weights and biases of the hidden layer,
  \item $W^{(2)} \in \mathbb{R}^{K \times H}$ and $b^{(2)} \in \mathbb{R}^{K}$ are the weights and biases of the output layer,
  \item $\sigma(\cdot)$ is a non-linear activation function such as ReLU or sigmoid,
  \item $\text{softmax}$ is defined as
  \[
  \text{softmax}(z)_k = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}}
  \]
  which converts raw scores into a valid probability distribution over $K$ classes.
\end{itemize}

\subsection*{Loss Function}
For classification, the most common loss is the cross-entropy loss. Given the true one-hot encoded label $y$ and predicted probabilities $\hat{y}$, the loss for a single sample is:
\[
\mathcal{L}(y,\hat{y}) = - \sum_{k=1}^{K} y_k \log \hat{y}_k
\]
Over a dataset of $m$ samples, we minimise the average loss:
\[
J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \mathcal{L}\left(y^{(i)}, \hat{y}^{(i)}\right)
\]
where $\theta$ denotes all weights and biases in the network.

\subsection*{Backpropagation and Gradient Descent}
The optimisation problem is unconstrained; we want to find $\theta$ that minimises $J(\theta)$. This is done via gradient-based methods. Backpropagation efficiently applies the chain rule of calculus to compute gradients of $J$ with respect to all parameters. Gradient descent then updates the parameters as:
\[
\theta^{(t+1)} = \theta^{(t)} - \eta \nabla_{\theta} J(\theta^{(t)})
\]
where $\eta$ is the learning rate.

By iteratively applying forward passes to compute predictions and losses, and backward passes to compute gradients, the network gradually learns useful intermediate representations in the hidden layers that capture non-linear relationships in the data.

\section*{Application}
To demonstrate the practical use of an ANN, we apply it to the Cleveland Heart Disease Dataset (present \href{https://www.kaggle.com/datasets/cherngs/heart-disease-cleveland-uci}{\textcolor{blue}{here}}), which provides informations about different patients and whether they have heart disease or not. This is useful becuase heart disease is related to many different lifestyle choices and other factors as well:

The possible classes (column named "condition") are:
\begin{itemize}
  \item \texttt{0} -- no heart disease
  \item \texttt{1} -- heart disease
\end{itemize}

A few attributes are categorical and we perform one hot-encoding for these columns and turn them into numerical feature vectors suitable for ANN training. After preprocessing, we split the dataset into training and testing datasets and then initialise the model, train it and then give the accuracy score for the model.

\section*{Code}
We implement a simple feed-forward ANN from scratch in pure C.

\subsection*{Model Implementation}
The config file defines basic configurations for the neural network.
\lstset{style=general}
\lstinputlisting[language=c]{./C/config.h}

The core implementation is contained in \texttt{C/neural-network.c}.
\lstinputlisting[language=c]{./C/neural-network.c}

\subsection*{Dataset Preprocessing}
The file \texttt{clean.py} preprocesses the dataset and performs one hot encoding of a the ``sex" column as it is categorical data.
\lstinputlisting[language=c]{./clean.py}

\section*{Output}
We use the following command to compile the project.
\begin{lstlisting}[style = general, language = bash]
$ cc -O3 -march=native -ffast-math -funroll-loops -flto -std=c99 neural-network.c -o neural-network -lm -g
\end{lstlisting}

The following is a sample of the output produced:
\begin{lstlisting}[style = general]
Good enough loss achieved stopping early                                    

Time taken to train model: 0.001959
The accuracy of the trained model is: 90.756303%
\end{lstlisting}

\section*{Conclusion}
Through this experiment, it becomes clear how stacking simple linear transformations with non-linear activations enables Artificial Neural Networks to learn powerful decision boundaries. Unlike Naive Bayes, which relies on conditional independence assumptions, or Decision Trees, which encode explicit symbolic rules, ANNs implicitly discover distributed internal representations that adapt to the data during training.

The custom implementation, its simplicity and focus on educational clarity, performs competitively with the highly optimised \texttt{MLPClassifier} from \texttt{scikit-learn}. This highlights a broader lesson in Machine Learning: many state-of-the-art models are conceptually straightforward once their optimisation and representation-learning mechanisms are understood. Implementing these models from first principles not only demystifies them, but also builds deep intuition about how data, architecture, and training dynamics interact.

\end{document}

